{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to AI: NLP Sentiment Classification\n",
    "\n",
    "## Audrey Zhang\n",
    "## October 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\audre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\audre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the required packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import parfit.parfit as pf\n",
    "\n",
    "\n",
    "from itertools import compress\n",
    "import collections\n",
    "from contractions import contractions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['conv_id','utterance_idx','context','prompt','speaker_idx','utterance','selfeval','tags']\n",
    "# Importing the datasets\n",
    "train = pd.read_csv('./empatheticdialogues/train.csv', usecols=cols)\n",
    "valid = pd.read_csv('./empatheticdialogues/valid.csv', usecols=cols)\n",
    "test = pd.read_csv('./empatheticdialogues/test.csv', usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select {'sad', 'jealous', 'joyful', 'terrified'} categories\n",
    "\n",
    "unique_labels=['sad', 'jealous', 'joyful', 'terrified']\n",
    "train=train.loc[train.context.isin(unique_labels)]\n",
    "valid=valid.loc[valid.context.isin(unique_labels)]\n",
    "test=test.loc[test.context.isin(unique_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select \"utterance\" and \"context\" as  X and y\n",
    "\n",
    "X_train = train.utterance.copy()\n",
    "X_test = test.utterance.copy()\n",
    "X_valid = valid.utterance.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the train labels and map for SGD classifier\n",
    "label_mapper = {}\n",
    "num = 0\n",
    "for label in unique_labels:\n",
    "    label_mapper[label] = num\n",
    "    num += 1\n",
    "\n",
    "\n",
    "labels_train = list(train['context'])\n",
    "labels_encoded_train = []\n",
    "for label in labels_train:\n",
    "    labels_encoded_train.append(label_mapper[label])\n",
    "\n",
    "\n",
    "# Getting test labels\n",
    "labels_test = list(test['context'])\n",
    "labels_encoded_test = []\n",
    "for label in labels_test:\n",
    "    labels_encoded_test.append(label_mapper[label])\n",
    "labels_encoded_test = np.array(labels_encoded_test)\n",
    "\n",
    "\n",
    "# get validation labels\n",
    "\n",
    "labels_valid=list(valid['context'])\n",
    "labels_encoded_valid=[]\n",
    "for label in labels_valid:\n",
    "    labels_encoded_valid.append(label_mapper[label])\n",
    "labels_encoded_valid=np.array(labels_encoded_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "\n",
    "def rm_punctuations(ds):\n",
    "    dataset=ds.copy()\n",
    "    dataset=dataset.apply(lambda x: x.lower())\n",
    "    dataset=dataset.apply(lambda x: re.sub(r'\\W', ' ', x))\n",
    "    dataset=dataset.apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "    dataset=dataset.apply(lambda x: re.sub('_comma_', '', x))\n",
    "    dataset=dataset.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    return dataset\n",
    "\n",
    "train_features_cleaned=rm_punctuations(X_train)\n",
    "test_features_cleaned=rm_punctuations(X_test)\n",
    "valid_features_cleaned=rm_punctuations(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to BOW model \n",
    "\n",
    "train_count_vectorizer = CountVectorizer()\n",
    "X = train_count_vectorizer.fit_transform(train_features_cleaned)\n",
    "encoding = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to      2862\n",
      "it      2661\n",
      "you     2510\n",
      "that    2479\n",
      "the     2249\n",
      "my      2165\n",
      "and     1795\n",
      "was     1697\n",
      "is      1520\n",
      "of      1369\n",
      "so      1358\n",
      "have    1167\n",
      "for     1069\n",
      "in      1068\n",
      "but      879\n",
      "me       877\n",
      "be       838\n",
      "am       805\n",
      "are      754\n",
      "just     740\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Converting counts to binary result for sparse encoding\n",
    "for arr in encoding:\n",
    "    arr[arr > 0] = 1\n",
    "\n",
    "print(pd.DataFrame(data=encoding, \n",
    "                   columns=train_count_vectorizer.get_feature_names()).sum(axis=0).sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of stopwords and appending additional words to it\n",
    "stopwords_list = list(set(stopwords.words('english')))\n",
    "stopwords_list.extend(['comma'])\n",
    "stopwords_list.remove('why') # removing this one because there are observations with only 'why' as the single word in the utterance\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the tokens in the stopwords list from utterance\n",
    "\n",
    "train_data_stop_removed = train_features_cleaned.apply(lambda x: ' '.join(\n",
    "     lemmatizer.lemmatize(i) for i in x.split() if i not in stopwords_list))\n",
    "\n",
    "test_data_stop_removed = test_features_cleaned.apply(lambda x: ' '.join(\n",
    "     lemmatizer.lemmatize(i) for i in x.split() if i not in stopwords_list))\n",
    "\n",
    "valid_data_stop_removed = valid_features_cleaned.apply(lambda x: ' '.join(\n",
    "    lemmatizer.lemmatize(i) for i in x.split() if i not in stopwords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also expand contractions  \n",
    "\n",
    "def expand_contractions(text):\n",
    "    for i in text.split():\n",
    "        if i in contractions_dict.keys():\n",
    "            text=re.sub(i, contractions_dict[i], text)\n",
    "    return text\n",
    "    \n",
    "train_data_stop_removed=train_data_stop_removed.apply(lambda x: expand_contractions(x)).reset_index(drop=True)\n",
    "test_data_stop_removed=test_data_stop_removed.apply(lambda x: expand_contractions(x))\n",
    "valid_data_stop_removed=valid_data_stop_removed.apply(lambda x: expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty_idx=[]\n",
    "#for i in range(len(train_data_stop_removed)):\n",
    "    #if len(train_data_stop_removed[i])==0:\n",
    "        #empty_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_stop_removed.drop(empty_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in empty_idx:\n",
    "    #labels_encoded_train.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9754\n",
      "9754\n"
     ]
    }
   ],
   "source": [
    "# confirm the train x and y are same size\n",
    "#print(len(labels_encoded_train))\n",
    "#print(len(train_data_stop_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the bag of words encoding again  \n",
    "train_count_vectorizer = CountVectorizer()\n",
    "X_vect = train_count_vectorizer.fit_transform(train_data_stop_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_encoding = X_vect.toarray()\n",
    "\n",
    "for arr in train_one_hot_encoding:\n",
    "    arr[arr > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get       686\n",
      "oh        619\n",
      "really    591\n",
      "time      570\n",
      "like      568\n",
      "friend    491\n",
      "good      473\n",
      "one       468\n",
      "got       452\n",
      "know      421\n",
      "sorry     378\n",
      "happy     374\n",
      "well      359\n",
      "go        359\n",
      "going     358\n",
      "year      356\n",
      "would     354\n",
      "day       349\n",
      "hope      347\n",
      "think     345\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check that this worked\n",
    "\n",
    "print(pd.DataFrame(data=train_one_hot_encoding, \n",
    "                   columns=train_count_vectorizer.get_feature_names()).sum(axis=0).sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           idf_weights\n",
      "get           3.653201\n",
      "oh            3.755816\n",
      "really        3.802029\n",
      "time          3.838146\n",
      "like          3.841655\n",
      "...                ...\n",
      "pinscher      9.492388\n",
      "pipe          9.492388\n",
      "piper         9.492388\n",
      "giggling      9.492388\n",
      "nightlife     9.492388\n",
      "\n",
      "[6226 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the training data using tfidf transformer \n",
    "\n",
    "train_tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "train_embedding_tfidf_transformer = train_tfidf_transformer.fit_transform(train_one_hot_encoding)\n",
    "\n",
    "# verify\n",
    "df_idf=pd.DataFrame(train_tfidf_transformer.idf_, index=train_count_vectorizer.get_feature_names(), columns=['idf_weights'])\n",
    "print(df_idf.sort_values(by=['idf_weights']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign X_train and y_train\n",
    "\n",
    "X_train = train_embedding_tfidf_transformer\n",
    "y_train = np.array(labels_encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sgd classifier \n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8143325815050236\n"
     ]
    }
   ],
   "source": [
    "# evaluate the training accuracy \n",
    "y_pred_train=clf.predict(X_train)\n",
    "#print(f1_score(y_train, y_pred_train, average='weighted'))\n",
    "print(accuracy_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on validation dataset for baseline\n",
    "valid_count_vectorizer=CountVectorizer(vocabulary=train_count_vectorizer.get_feature_names())\n",
    "X_valid=valid_count_vectorizer.fit_transform(valid_data_stop_removed)\n",
    "y_valid=np.array(labels_encoded_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_one_hot_encoding=X_valid.toarray()\n",
    "for arr in valid_one_hot_encoding:\n",
    "    arr[arr > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\audre\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1447: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "valid_tfidf_transformer = TfidfTransformer(smooth_idf=False,use_idf=True)\n",
    "valid_embedding_tfidf_transformer = valid_tfidf_transformer.fit_transform(valid_one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6312741312741312\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions on test data\n",
    "y_pred_valid = clf.predict(valid_embedding_tfidf_transformer)\n",
    "# SGD_report=classification_report(y_valid, y_pred_valid, target_names=unique_labels)\n",
    "# print(SGD_report)\n",
    "#print(f1_score(y_valid, y_pred_valid, average='weighted'))\n",
    "print(accuracy_score(y_valid, y_pred_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid={\n",
    "      'loss':['hinge', 'log', 'modified_huber', 'perceptron'],\n",
    "      'penalty':['l1', 'l2', 'elasticnet'],\n",
    "      'alpha': [0.01, 0.001, 0.0001, 0.00001],\n",
    "      'max_iter': [200, 500, 1000, 5000],\n",
    "      'learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
    "      'eta0': [0.1, 0.5]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------FITTING MODELS-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1999s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 266 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0894s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done 302 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 325 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 346 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 369 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 392 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 469 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 525 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done 554 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 585 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 649 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   35.8s\n",
      "[Parallel(n_jobs=-1)]: Done 717 tasks      | elapsed:   39.9s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:   42.1s\n",
      "[Parallel(n_jobs=-1)]: Done 789 tasks      | elapsed:   45.0s\n",
      "[Parallel(n_jobs=-1)]: Done 826 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-1)]: Done 865 tasks      | elapsed:   52.6s\n",
      "[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:   55.6s\n",
      "[Parallel(n_jobs=-1)]: Done 945 tasks      | elapsed:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done 986 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1029 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1072 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1117 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------SCORING MODELS-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many dimensions to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:    3.2s finished\n"
     ]
    }
   ],
   "source": [
    "paramGrid=ParameterGrid(grid)\n",
    "bestModel, bestScore, allModels, allScores=pf.bestFit(SGDClassifier, paramGrid,\n",
    "                                                      X_train, y_train, X_valid, y_valid,\n",
    "                                                      metric=accuracy_score, greater_is_better=True, scoreLabel='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=1e-05, eta0=0.1, learning_rate='adaptive', loss='log',\n",
      "              max_iter=200) 0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "print(bestModel, bestScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.1, learning_rate='adaptive', loss='log')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best accuracy score on validation dataset achieved is 64.2% \n",
    "# with model parameters alpha=1e-05, eta0=0.1, learning_rate='adaptive', loss='log'\n",
    "\n",
    "#%%\n",
    "# re-train model on test set with best model params identified above\n",
    "clf1 = SGDClassifier(alpha=0.0001, eta0=0.1, learning_rate='adaptive', loss='log')\n",
    "clf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test data \n",
    "\n",
    "test_count_vectorizer = CountVectorizer(vocabulary = train_count_vectorizer.get_feature_names())\n",
    "X_test = test_count_vectorizer.fit_transform(test_data_stop_removed)\n",
    "y_test=labels_encoded_test\n",
    "test_one_hot_encoding = X_test.toarray()\n",
    "\n",
    "for arr in test_one_hot_encoding:\n",
    "    arr[arr > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\audre\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1447: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the test data  \n",
    "test_tfidf_transformer = TfidfTransformer(smooth_idf=False,use_idf=True)\n",
    "test_embedding_tfidf_transformer = test_tfidf_transformer.fit_transform(test_one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions on test data\n",
    "y_pred_test = clf1.predict(test_embedding_tfidf_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.6143790849673203\n"
     ]
    }
   ],
   "source": [
    "# do some evaluation on the test set\n",
    "print('Test accuracy :', np.mean(y_test == y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.6143790849673203\n"
     ]
    }
   ],
   "source": [
    "f1_score_vector = f1_score(y_test, y_pred_test, average=None)\n",
    "print('F1 score :', np.mean(y_test == y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "            pred_sad  pred_jealous  pred_joyful  pred_terrified\n",
      "sad             230            44           50              50\n",
      "jealous          57           181           62              50\n",
      "joyful           63            40          220              32\n",
      "terrified        35            28           20             215\n",
      "f1 score using SGD classifier is : 0.6145153062295787\n"
     ]
    }
   ],
   "source": [
    "conf_mat=pd.DataFrame(confusion_matrix(y_test, y_pred_test), \n",
    "                      index=unique_labels, columns=['pred_'+i for i in unique_labels])\n",
    "print('Confusion matrix :\\n', conf_mat)\n",
    "\n",
    "print('f1 score using SGD classifier is :', np.mean(f1_score_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix normalized:\n",
      "            pred_sad  pred_jealous  pred_joyful  pred_terrified\n",
      "sad        0.614973      0.117647     0.133690        0.133690\n",
      "jealous    0.162857      0.517143     0.177143        0.142857\n",
      "joyful     0.177465      0.112676     0.619718        0.090141\n",
      "terrified  0.117450      0.093960     0.067114        0.721477\n"
     ]
    }
   ],
   "source": [
    "conf_mat_norm=conf_mat.div(conf_mat.sum(axis=1), axis=0)\n",
    "print('Confusion matrix normalized:\\n', conf_mat_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            utterance  context pred_context\n",
      "1   Ugh_comma_ those articles always get me too......      sad      jealous\n",
      "4   yes! And i do believe in God and prayers but g...      sad    terrified\n",
      "6                   3 years is a long time. How come?   joyful          sad\n",
      "8            Oh I see. They must miss you_comma_ too.   joyful          sad\n",
      "14  She was around 11_comma_ so she took it very h...      sad    terrified\n",
      "16  One of the saddest things to me is when people...      sad      jealous\n",
      "18  That's perfectly natural. You sound like the k...      sad       joyful\n",
      "20  I met up with an old flame recently_comma_ did...  jealous          sad\n",
      "21               Oh ya? What happened?? I'm intrigued  jealous    terrified\n",
      "23  Woah plot twist. She brought him along to meet...  jealous       joyful\n"
     ]
    }
   ],
   "source": [
    "# analyze some misclassified samples\n",
    "\n",
    "misclassified=list(np.where(y_test!=y_pred_test)[0])\n",
    "y_label_test=np.array([unique_labels[i] for i in y_pred_test]).transpose()\n",
    "df_y_pred=pd.DataFrame(y_label_test, columns=['pred_context'])\n",
    "df_x=test[['utterance', 'context']].copy().reset_index(drop=True)\n",
    "inaccurates=df_x.merge(df_y_pred, left_index=True, right_index=True).iloc[misclassified]\n",
    "print(inaccurates.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaccurates.to_csv('inaccurate_classifications.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier using pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the data\n",
    "train_tokens = [nltk.word_tokenize(sentences) for sentences in train_data_stop_removed]\n",
    "train_y = np.array(labels_encoded_train)\n",
    "\n",
    "test_tokens = [nltk.word_tokenize(sentences) for sentences in test_data_stop_removed]\n",
    "test_y = np.array(labels_encoded_test)\n",
    "\n",
    "valid_tokens=[nltk.word_tokenize(sentences) for sentences in valid_data_stop_removed]\n",
    "valid_y=np.array(labels_encoded_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained word2vec model from Google\n",
    "# download the model here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(w2v_model, sentence):\n",
    "    sentence = [word for word in sentence if word in w2v_model.vocab]\n",
    "    return np.mean(w2v_model[sentence], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will help us drop documents that have no word vectors in word2vec\n",
    "def has_vector_representation(word2vec_model, sentence):\n",
    "    return not all(word not in word2vec_model.vocab for word in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes input of tokens and output fixed length numerical vector, with sentence-level mbedding averaged\n",
    "x=[]\n",
    "unmatched_idx=[] #keep track of samples with no matching words in w2v model\n",
    "for i in range(len(train_tokens)):\n",
    "    sent=train_tokens[i]\n",
    "    if has_vector_representation(model, sent):\n",
    "        x.append(list(document_vector(model, sent)))\n",
    "    else:\n",
    "        unmatched_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unmatched_idx:\n",
    "    train_y=np.delete(train_y, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9746\n",
      "9746\n"
     ]
    }
   ],
   "source": [
    "print(len(train_y))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372\n",
      "1372\n"
     ]
    }
   ],
   "source": [
    "# transform test data for MLP model prediction\n",
    "test_x=[]\n",
    "unmatched_idx_test=[] #keep track of samples with no matching words in w2v model\n",
    "for i in range(len(test_tokens)):\n",
    "    sent=test_tokens[i]\n",
    "    if has_vector_representation(model, sent):\n",
    "        test_x.append(list(document_vector(model, sent)))\n",
    "    else:\n",
    "        unmatched_idx_test.append(i)\n",
    "        \n",
    "for i in unmatched_idx_test:\n",
    "    test_y=np.delete(test_y, i)\n",
    "    \n",
    "print(len(test_x))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544\n",
      "1544\n"
     ]
    }
   ],
   "source": [
    "# transform validation data for MLP model prediction\n",
    "valid_x=[]\n",
    "unmatched_idx_val=[] #keep track of samples with no matching words in w2v model\n",
    "for i in range(len(valid_tokens)):\n",
    "    sent=valid_tokens[i]\n",
    "    if has_vector_representation(model, sent):\n",
    "        valid_x.append(list(document_vector(model, sent)))\n",
    "    else:\n",
    "        unmatched_idx_val.append(i)\n",
    "        \n",
    "for i in unmatched_idx_val:\n",
    "    valid_y=np.delete(valid_y, i)\n",
    "    \n",
    "print(len(valid_x))\n",
    "print(len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf=MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5680051813471503"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf.fit(x, train_y)\n",
    "val_y_pred=mlp_clf.predict(valid_x)\n",
    "accuracy_score(valid_y, val_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust hyperparams for better performance \n",
    "mlp_clf=MLPClassifier(hidden_layer_sizes=(200, ), max_iter=400, activation='tanh', solver='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(200,), max_iter=400,\n",
       "              solver='sgd')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf.fit(x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y_pred=mlp_clf.predict(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6262953367875648"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_y, val_y_pred)\n",
    "# this model performs slightly better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test accuracy score is:  0.6297376093294461\n"
     ]
    }
   ],
   "source": [
    "test_y_pred=mlp_clf.predict(test_x)\n",
    "print(\"the test accuracy score is: \", accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "            pred_sad  pred_jealous  pred_joyful  pred_terrified\n",
      "sad             235            56           40              41\n",
      "jealous          55           196           66              31\n",
      "joyful           58            49          212              35\n",
      "terrified        31            21           25             221\n",
      "f1 score using SGD classifier is : 0.6313246288596899\n"
     ]
    }
   ],
   "source": [
    "conf_mat=pd.DataFrame(confusion_matrix(test_y, test_y_pred), \n",
    "                      index=unique_labels, columns=['pred_'+i for i in unique_labels])\n",
    "print('Confusion matrix :\\n', conf_mat)\n",
    "\n",
    "f1_score_vector=f1_score(test_y, test_y_pred, average=None)\n",
    "print('f1 score using SGD classifier is :', np.mean(f1_score_vector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
